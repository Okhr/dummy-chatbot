Collector:
  active: false #set to 'false' to skip this step
  subreddit: 'explainlikeimfive'
  time_filters: ['all', 'year', 'month', 'week', 'day']

Pairs:
  active: false #set to 'false' to skip this step
  top_comments: true #whether to generate top pairs in addition to "all" pairs

Tokenizer:
  training_active: false #whether to train a new tokenizer based on the following parameters or not
  dataset_making_active: false #whether to write pair datasets to disk (lstm and transformer formatted) or not
  vocab_size: 5000
  max_seq_length: 256

Training:
  active: true
  model: 'new' #'new' to train a new model or <model_name> to retrain an already existing one
  model_type: 'transformer' #'lstm' or 'transformer', only useful when training a new model
  dataset_type: 'all' #'top' or 'all'
  lstm:
    embedding_dim: 256
    hidden_dim: 128
    n_layers: 2
    dropout: 0.3
  transformer:
    d_model: 128
    n_heads: 4
    hidden_dim: 512
    n_layers: 6
    dropout: 0.5
  epochs: 100
  batch_size: 64

Inference:
  active: true
  model_name: 'last' #'last' to use the last trained model or <model_name>